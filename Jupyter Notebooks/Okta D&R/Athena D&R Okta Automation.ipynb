{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "74ef99ef-9872-4e45-9024-76c28dc31e9e",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "# Athena D&R Okta Automation\n",
        "This notebook outlines the process of setting up automated Athena queries for security detections. We'll be focusing on ingesting logs into AWS Security Lake and creating detections based on those logs.\n",
        "## Logs to Ingest\n",
        "- **Tier 1: AWS Environment**\n",
        "  - AWS CloudTrail logs\n",
        "  - AWS GuardDuty findings\n",
        "  - AWS S3 access logs\n",
        "  - AWS WAF\n",
        "  - AWS CloudWatch logs\n",
        "- **Tier 1: SaaS Applications**\n",
        "  - Okta logs\n",
        "  - GitHub audit logs\n",
        "  - Postman App Logs\n",
        "  - Postman Cloudflare Logs\n",
        "## Additional Tools\n",
        "- AWS Glue and Brex Substation for log ingestion, transformation, and enrichment.\n",
        "## Objective\n",
        "To create scheduled Athena queries that will function as detections and run every minute. These detections will be based on the DDL table `postman_s3_okta_audit_logs`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ef30a5-5e39-4ef1-ba9d-6bdf5d6d1867",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Athena Queries for Detections\n",
        "Here are some example Athena queries that can be used for detections. These queries are based on the DDL table `postman_s3_okta_audit_logs` and are designed to detect suspicious activities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3401f1a9-d0e8-43b9-aa04-c3aa099fdbeb",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Initialize Athena client\n",
        "athena_client = boto3.client('athena', region_name='us-east-1')\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client('s3', region_name='us-east-1')\n",
        "\n",
        "# Athena settings\n",
        "athena_database = 'your_database'  # Replace with your Athena database name\n",
        "athena_output_bucket = 's3://your-athena-output-bucket/'  # Replace with your S3 bucket where Athena will store query results\n",
        "\n",
        "# Function to run Athena query\n",
        "def run_athena_query(query):\n",
        "    response = athena_client.start_query_execution(\n",
        "        QueryString=query,\n",
        "        QueryExecutionContext={\n",
        "            'Database': athena_database\n",
        "        },\n",
        "        ResultConfiguration={\n",
        "            'OutputLocation': athena_output_bucket\n",
        "        }\n",
        "    )\n",
        "    query_execution_id = response['QueryExecutionId']\n",
        "    return query_execution_id\n",
        "\n",
        "# Function to check Athena query status\n",
        "def check_athena_query_status(query_execution_id):\n",
        "    response = athena_client.get_query_execution(\n",
        "        QueryExecutionId=query_execution_id\n",
        "    )\n",
        "    status = response['QueryExecution']['Status']['State']\n",
        "    return status\n",
        "\n",
        "# Function to get Athena query results\n",
        "def get_athena_query_results(query_execution_id):\n",
        "    results = []\n",
        "    try:\n",
        "        response = athena_client.get_query_results(\n",
        "            QueryExecutionId=query_execution_id\n",
        "        )\n",
        "        for row in response['ResultSet']['Rows'][1:]:  # Skip header row\n",
        "            results.append(row['Data'])\n",
        "    except ClientError as e:\n",
        "        print(f'An error occurred: {e}')\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff309b8-6a69-4b3a-b676-cd9340fda0d8",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Athena Queries\n",
        "The following Athena queries are designed to detect suspicious activities based on the Okta logs stored in the `postman_s3_okta_audit_logs` table. These queries will be executed by the AWS Lambda function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e454c4b-8311-4638-88f2-b41b2b0071b5",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Define Athena queries for detections\n",
        "queries = {\n",
        "    'suspicious_ips': '''\n",
        "    SELECT detail.client.ipaddress AS suspicious_ip, COUNT(*) AS count\n",
        "    FROM your_database.postman_s3_okta_audit_logs\n",
        "    WHERE detail.outcome.result = 'FAILURE'\n",
        "    GROUP BY detail.client.ipaddress\n",
        "    HAVING COUNT(*) > 5\n",
        "    ''',\n",
        "    'unusual_user_agents': '''\n",
        "    SELECT detail.client.useragent.rawuseragent AS user_agent, COUNT(*) AS count\n",
        "    FROM your_database.postman_s3_okta_audit_logs\n",
        "    GROUP BY detail.client.useragent.rawuseragent\n",
        "    HAVING COUNT(*) < 3\n",
        "    ''',\n",
        "    'high_frequency_failed_logins': '''\n",
        "    SELECT detail.actor.id AS user_id, COUNT(*) AS failed_count\n",
        "    FROM your_database.postman_s3_okta_audit_logs\n",
        "    WHERE detail.outcome.result = 'FAILURE'\n",
        "    GROUP BY detail.actor.id\n",
        "    HAVING COUNT(*) > 10\n",
        "    ''',\n",
        "    'unusual_times_of_activity': '''\n",
        "    SELECT date_parse(time, '%Y-%m-%dT%H:%i:%s.%fZ') AS parsed_time, COUNT(*) AS count\n",
        "    FROM your_database.postman_s3_okta_audit_logs\n",
        "    WHERE date_format(date_parse(time, '%Y-%m-%dT%H:%i:%s.%fZ'), '%H') NOT BETWEEN '08' AND '18'\n",
        "    GROUP BY date_parse(time, '%Y-%m-%dT%H:%i:%s.%fZ')\n",
        "    ''',\n",
        "    'unusual_geographical_locations': '''\n",
        "    SELECT detail.client.geographicalcontext.country AS country, COUNT(*) AS count\n",
        "    FROM your_database.postman_s3_okta_audit_logs\n",
        "    GROUP BY detail.client.geographicalcontext.country\n",
        "    HAVING COUNT(*) < 5\n",
        "    '''\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2979c61-6559-4063-89c8-c192555febc9",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## AWS Lambda Function\n",
        "We'll create an AWS Lambda function to execute these Athena queries. The Lambda function will be triggered by AWS CloudWatch Events every minute to run all the detections. The results will be stored in another Athena table or sent to a monitoring system for alerting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd57162b-a6cb-4d94-822a-3df2d5cd72f4",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Lambda function to execute Athena queries\n",
        "def lambda_handler(event, context):\n",
        "    for query_name, query in queries.items():\n",
        "        print(f'Running query: {query_name}')\n",
        "        query_execution_id = run_athena_query(query)\n",
        "        while True:\n",
        "            status = check_athena_query_status(query_execution_id)\n",
        "            if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
        "                break\n",
        "            time.sleep(5)  # Wait for 5 seconds before checking the status again\n",
        "        if status == 'SUCCEEDED':\n",
        "            results = get_athena_query_results(query_execution_id)\n",
        "            print(f'Results for {query_name}: {results}')\n",
        "            # TODO: Store results in another Athena table or send to monitoring system\n",
        "        else:\n",
        "            print(f'Query {query_name} failed to execute')\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': json.dumps('Athena queries executed successfully.')\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca09a0de-32f7-45d6-ad80-e9560260ff11",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## GitHub Actions Workflow\n",
        "To keep the queries up-to-date, we can use GitHub Actions to automatically update the Lambda function whenever the queries are updated in the GitHub repository. The workflow will do the following:\n",
        "- Check out the latest code from the GitHub repository\n",
        "- Install AWS CLI\n",
        "- Update the Lambda function with the new queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63fdbec0-bd14-4e87-91d1-60bd45899e1b",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# GitHub Actions YAML configuration for updating Lambda function\n",
        "github_actions_yaml = '''\n",
        "name: Update Lambda Function\n",
        "\\non: [push]\n",
        "\\njobs:\n",
        "  update-lambda:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "    - name: Checkout code\n",
        "      uses: actions/checkout@v2\n",
        "    - name: Set up AWS CLI\n",
        "      run: pip install awscli\n",
        "    - name: Update Lambda function\n",
        "      run: aws lambda update-function-code --function-name your-lambda-function-name --zip-file fileb://your-code.zip\n",
        "      env:\n",
        "        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
        "        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
        "        AWS_REGION: us-east-1\n",
        "'''\n",
        "print(github_actions_yaml)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "identifier": "legacy",
      "language": "python",
      "language_version": "3.9",
      "name": "python3"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "0fe2e485-ec91-5d3d-8294-34c1e81dd91b",
        "openai_ephemeral_user_id": "1a6ecd3a-644b-50e4-88db-f0ee358f9201",
        "openai_subdivision1_iso_code": "US-TN"
      }
    },
    "selected_hardware_size": "small"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
